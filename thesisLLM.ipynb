{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-x37EMQ699yN"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os, re, json, logging\n",
        "from typing import List, Dict\n",
        "import fitz          \n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzkZlGtqBdg-"
      },
      "source": [
        "#find table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sTHym9yaBYgo"
      },
      "outputs": [],
      "source": [
        "# ---------- Page discovery utilities ----------\n",
        "MIN_CODES = 5                                           # tweak if a report is sparse\n",
        "CODE_RE   = re.compile(r'\\b(?:BP|GOV|SBM|IRO|E[1-5]|S[1-4]|G[1-3])[-–‐]?\\d{1,2}\\b', re.I)\n",
        "\n",
        "def code_cnt(txt: str) -> int:\n",
        "    \"Count ESRS-style code tokens in a block of text.\"\n",
        "    return len(CODE_RE.findall(txt))\n",
        "\n",
        "def find_esrs_pages(pdf_path: str) -> List[int]:\n",
        "    \"\"\"\n",
        "    Return a **1-based** list of pages that most likely hold the ESRS\n",
        "    cross-reference table, using the heuristic you provided.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "\n",
        "    # 1️⃣ “ESRS table of contents” segment\n",
        "    toc_hits = [\n",
        "        i for i, p in enumerate(doc)\n",
        "        if \"esrs table of contents\" in p.get_text().lower()\n",
        "        and code_cnt(p.get_text()) >= MIN_CODES\n",
        "    ]\n",
        "    if toc_hits:\n",
        "        start  = toc_hits[-1]\n",
        "        pages  = [start]\n",
        "        for j in range(start + 1, len(doc)):\n",
        "            if code_cnt(doc[j].get_text()) >= MIN_CODES:\n",
        "                pages.append(j)\n",
        "            else:\n",
        "                break\n",
        "        return [p + 1 for p in pages]                     # 1-based return\n",
        "\n",
        "    # 2️⃣ densest contiguous block anywhere in the PDF\n",
        "    qualifies = [code_cnt(p.get_text()) >= MIN_CODES for p in doc]\n",
        "    blocks, i = [], 0\n",
        "    while i < len(doc):\n",
        "        if not qualifies[i]:\n",
        "            i += 1; continue\n",
        "        start = i\n",
        "        while i < len(doc) and qualifies[i]:\n",
        "            i += 1\n",
        "        blocks.append(range(start, i))\n",
        "    if not blocks:\n",
        "        return []\n",
        "\n",
        "    best = max(blocks, key=lambda r: len(r)*sum(code_cnt(doc[p].get_text()) for p in r))\n",
        "    return [p + 1 for p in best]                          # 1-based list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QOzBE_LbC1wN",
        "outputId": "63c8dfa0-8a15-4647-db61-6b89bb18dd54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8.7 \n",
            "ESRS cross-reference table\n",
            "ESRS 2 \n",
            "General disclosures\n",
            "Disclosure Requirement BP-1 – General basis for preparation of sustainability\n",
            "Sustainability statement: General basis for preparation\n",
            "169 Limited Assurance\n",
            "Disclosure Requirement BP-2 – Disclosures in relation to specific circumstances\n",
            "Sustainability statement: General basis for preparation\n",
            "169 Limited Assurance\n",
            "Disclosure Requirement GOV-1 – The role of the administrative, management and \n",
            "supervisory bodies\n",
            "SFDR/BRR\n",
            "Environmental, Social and Governance: ESG governance\n",
            "Supervisory Board report: composition, diversity and self-evaluation\n",
            "Other Board-related matters: Diversity\n",
            "40\n",
            "64\n",
            "268\n",
            "Limited Assurance\n",
            "Disclosure Requirement GOV-2 – Information provided to and sustainability matters \n",
            "addressed by the undertaking’s administrative, management and supervisory bodies\n",
            "Environmental, Social and Governance: ESG governance\n",
            "40 Limited Assurance\n",
            "Disclosure Requirement GOV-3 - Integration of sustainability-related performance in incentiv\n"
          ]
        }
      ],
      "source": [
        "import fitz\n",
        "\n",
        "try:\n",
        "    # Update path to local Reports directory\n",
        "    doc = fitz.open(\"Reports/PhilipsFullAnnualReport2024-English.pdf\")\n",
        "    text = doc[242].get_text()  # zero-indexed\n",
        "    print(text[:1000])  # show first 1000 characters\n",
        "finally:\n",
        "    # Ensure document is closed\n",
        "    if 'doc' in locals():\n",
        "        doc.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUZEKrZbDAAa",
        "outputId": "4fdaa82d-0e06-43fa-b3ab-f01480dc6913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ESRS pages: [243, 244, 245, 246, 247]\n"
          ]
        }
      ],
      "source": [
        "pdf_path = \"Reports/PhilipsFullAnnualReport2024-English.pdf\"\n",
        "pages = find_esrs_pages(pdf_path)\n",
        "print(\"ESRS pages:\", pages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7wYHxeoCquJ"
      },
      "source": [
        "#LLM APY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PNlOBe4XBceo"
      },
      "outputs": [],
      "source": [
        "# ---------- LLM extraction ----------\n",
        "load_dotenv()  # Load environment variables\n",
        "\n",
        "api_key = os.getenv('OPENAI_API_KEY')\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "def extract_page_items(text: str, page_num: int, client: OpenAI) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Call the model once for a single page of text and return list[dict].\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are an ESRS disclosure analyzer. Extract **all** ESRS disclosures\n",
        "from the text below (page {page_num}).  Return JSON like:\n",
        "{{\"items\":[{{\"code\":\"\",\"title\":\"\",\"page_reference\":\"\"}}]}}.\n",
        "\n",
        "Text:\n",
        "{text}\"\"\"\n",
        "    try:\n",
        "        resp  = client.chat.completions.create(\n",
        "            model=\"gpt-4.1\",\n",
        "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "            response_format={\"type\":\"json_object\"},\n",
        "        )\n",
        "        data  = json.loads(resp.choices[0].message.content)\n",
        "        for itm in data.get(\"items\", []):\n",
        "            itm[\"source_page\"] = page_num\n",
        "        return data.get(\"items\", [])\n",
        "    except Exception as e:\n",
        "        logging.error(f\"LLM error on page {page_num}: {e}\")\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ---------- LLM extraction ----------\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# constants for the final table\n",
        "COMPANY        = \"Philips\"   # or inject dynamically\n",
        "CATEGORY       = \"DR\"\n",
        "MAX_PAGE_COLS  = 5           # Page_ref1 … Page_ref5\n",
        "\n",
        "def extract_page_items(text: str, page_num: int, client: OpenAI) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Call GPT once for a single page and return a **ready-made DataFrame**\n",
        "    with columns:  name, category, variable, value,\n",
        "                   Page_ref1 … Page_ref{MAX_PAGE_COLS}\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are an ESRS disclosure analyzer. Extract **all** ESRS disclosures\n",
        "from the text below (page {page_num}).  Return JSON exactly like:\n",
        "{{\"items\":[{{\"code\":\"\",\"title\":\"\",\"page_reference\":\"\"}}]}}  — no extra keys.\n",
        "\n",
        "Text:\n",
        "{text}\"\"\"\n",
        "\n",
        "    try:\n",
        "        resp  = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",   # or \"gpt-4o\" / \"gpt-4.1\"\n",
        "            messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "            response_format={\"type\":\"json_object\"},\n",
        "        )\n",
        "        data = json.loads(resp.choices[0].message.content)\n",
        "        items: List[Dict] = data.get(\"items\", [])\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"LLM error on page {page_num}: {e}\")\n",
        "        items = []\n",
        "\n",
        "    # ---------- reshape to “ideal” layout ----------\n",
        "    if not items:         # nothing found on that page\n",
        "        return pd.DataFrame(columns=[\"name\",\"category\",\"variable\",\"value\"]\n",
        "                                      + [f\"Page_ref{i}\" for i in range(1, MAX_PAGE_COLS+1)])\n",
        "\n",
        "    df = pd.DataFrame(items)\n",
        "\n",
        "    # add constant / derived columns\n",
        "    df = (df\n",
        "          .assign(\n",
        "              name     = COMPANY,\n",
        "              category = CATEGORY,\n",
        "              variable = df[\"code\"],\n",
        "              value    = 1,                     # 1 = disclosed\n",
        "          )\n",
        "          .drop(columns=[\"title\"])              # keep if you still need it elsewhere\n",
        "    )\n",
        "\n",
        "    # split “76, 80” → Page_ref1 … Page_refN\n",
        "    pages = (df[\"page_reference\"]\n",
        "             .str.split(\",\", expand=True)\n",
        "             .apply(lambda col: col.str.strip())\n",
        "             .rename(columns=lambda i: f\"Page_ref{i+1}\"))\n",
        "\n",
        "    # ensure we always have Page_ref1 … Page_ref{MAX_PAGE_COLS}\n",
        "    for i in range(pages.shape[1]+1, MAX_PAGE_COLS+1):\n",
        "        pages[f\"Page_ref{i}\"] = \"\"\n",
        "\n",
        "    df = pd.concat([df.drop(columns=[\"page_reference\"]), pages], axis=1)\n",
        "\n",
        "    # final column order\n",
        "    df = df[[\"name\",\"category\",\"variable\",\"value\"]\n",
        "            + [f\"Page_ref{i}\" for i in range(1, MAX_PAGE_COLS+1)]]\n",
        "\n",
        "    return df.fillna(\"\")        # blanks instead of NaN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n7ji2npyDOMe",
        "outputId": "7ebab3c2-2d94-4cbf-b138-d9868284971c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chosen pages: [243, 244, 245, 246, 247]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "cannot concatenate object of type '<class 'list'>'; only Series and DataFrame objs are valid",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     df_page = extract_page_items(text, p, client)\n\u001b[32m     12\u001b[39m     dfs.append(df_page)                 \u001b[38;5;66;03m# << append, do NOT extend\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m full_table = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m display(full_table.style.hide(axis=\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# full_table.to_excel(\"Philips_ESRS.xlsx\", index=False)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bachelor-thesis-group32/.venv/lib/python3.13/site-packages/pandas/core/reshape/concat.py:382\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m op = \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bachelor-thesis-group32/.venv/lib/python3.13/site-packages/pandas/core/reshape/concat.py:448\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    445\u001b[39m objs, keys = \u001b[38;5;28mself\u001b[39m._clean_keys_and_objs(objs, keys)\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m ndims = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_ndims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m sample, objs = \u001b[38;5;28mself\u001b[39m._get_sample_object(objs, ndims, keys, names, levels)\n\u001b[32m    451\u001b[39m \u001b[38;5;66;03m# Standardize axis parameter to int\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/bachelor-thesis-group32/.venv/lib/python3.13/site-packages/pandas/core/reshape/concat.py:489\u001b[39m, in \u001b[36m_Concatenator._get_ndims\u001b[39m\u001b[34m(self, objs)\u001b[39m\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (ABCSeries, ABCDataFrame)):\n\u001b[32m    485\u001b[39m         msg = (\n\u001b[32m    486\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcannot concatenate object of type \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    487\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33monly Series and DataFrame objs are valid\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    488\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m    491\u001b[39m     ndims.add(obj.ndim)\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ndims\n",
            "\u001b[31mTypeError\u001b[39m: cannot concatenate object of type '<class 'list'>'; only Series and DataFrame objs are valid"
          ]
        }
      ],
      "source": [
        "pdf_path = \"Reports/PhilipsFullAnnualReport2024-English.pdf\"\n",
        "\n",
        "pages = find_esrs_pages(pdf_path)\n",
        "print(\"Chosen pages:\", pages)\n",
        "\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "dfs = []                                            # << DataFrames live here\n",
        "for p in pages:\n",
        "    text = doc[p-1].get_text()          # PyMuPDF uses 0-based index\n",
        "    df_page = extract_page_items(text, p, client)\n",
        "    dfs.append(df_page)                 # << append, do NOT extend\n",
        "\n",
        "full_table = pd.concat(dfs, ignore_index=True)\n",
        "display(full_table.style.hide(axis=\"index\"))\n",
        "# full_table.to_excel(\"Philips_ESRS.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulu3kyr7DNPu"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
